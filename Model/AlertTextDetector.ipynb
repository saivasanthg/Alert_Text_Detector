{ "cells": [ { "cell_type": "code", "execution_count": null, "metadata": {"id": "_tZo2YIpJWXV"}, "outputs": [], "source": [ "import pandas as pd\n", "import sklearn\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.preprocessing import LabelEncoder\n", "import numpy as np\n", "import torch\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "btxJ92g3J4bA"}, "outputs": [], "source": [ "df = pd.read_csv('path/to/train.csv', encoding='latin-1')\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "YbddypWkJ9y8", "outputId": "5d2fc861-f130-4a16-bce3-85c23e5585ba" }, "outputs": [ { "output_type": "stream", "name": "stdout", "text": [ "Positive samples: 10500\n", "Negative samples: 12498\n" ] } ], "source": [ "label_counts = df['target'].value_counts()\n", "\n", "positive_count = label_counts.get(1, 0)\n", "negative_count = label_counts.get(0, 0)\n", "\n", "print(f\"Positive samples: {positive_count}\")\n", "print(f\"Negative samples: {negative_count}\")" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "b8rEcCK5J_J8", "outputId": "6fb871ed-adb4-43c5-e732-570784d2c6fe" }, "outputs": [ { "output_type": "stream", "name": "stdout", "text": [ "\n", "RangeIndex: 22998 entries, 0 to 22997\n", "Data columns (total 5 columns):\n", " # Column Non-Null Count Dtype \n", "--- ------ -------------- ----- \n", " 0 id 7598 non-null float64\n", " 1 keyword 7538 non-null object \n", " 2 location 5070 non-null object \n", " 3 text 22998 non-null object \n", " 4 target 22998 non-null int64 \n", "dtypes: float64(1), int64(1), object(3)\n", "memory usage: 898.5+ KB\n" ] } ], "source": [ "df.info()\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "0zrrdlgRKAro"}, "outputs": [], "source": [ "#Converting Data to Dataframe and defining X and labels\n", "data_ = pd.DataFrame(df)\n", "x = data_.drop('target', axis=1)\n", "labels = df['target']" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "h3P-nfTlTnHU"}, "outputs": [], "source": [ "x['text'] = x['text'].str.replace(r'http\\S+|www\\S+|@\\S+', '', regex=True)\n", "x['hashtag'] = x['text'].str.findall(r'#(\\w+)').apply(lambda hashtags: ', '.join(hashtags) if hashtags else np.nan)\n", "x['text'] = x['text'].str.replace('#', '', regex=False)\n", "x['text_'] = x['text'].str.replace(\"[.,!]\", \"\", regex=True)\n", "x['text_'] = x['text_'].str.lower()\n", "x['hashtag'] = x['hashtag'].str.lower()\n", "x=x.drop(['id','text','location'],axis=1)" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "_gJVQo6PTpR3", "outputId": "0ddff8a8-156d-40fa-be5b-cd89fd3fdd9f" }, "outputs": [ { "output_type": "stream", "name": "stderr", "text": [ "[nltk_data] Downloading package stopwords to /root/nltk_data...\n", "[nltk_data] Unzipping corpora/stopwords.zip.\n", "[nltk_data] Downloading package wordnet to /root/nltk_data...\n", "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n", "[nltk_data] Unzipping tokenizers/punkt_tab.zip.\n" ] }, { "output_type": "execute_result", "data": { "text/plain": ["True"] }, "metadata": {}, "execution_count": 4 } ], "source": [ "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.stem import WordNetLemmatizer\n", "import pandas as pd\n", "nltk.download('stopwords')\n", "nltk.download('wordnet')\n", "nltk.download('punkt_tab')" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "OHpWQuBZTrb2"}, "outputs": [], "source": [ "lemmatizer = WordNetLemmatizer()\n", "stop_words = set(stopwords.words('english'))\n", "def preprocess_text(text):\n", " words = nltk.word_tokenize(text.lower()) # Tokenize and convert to lowercase\n", " words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words] # Use str.isalpha()\n", " return \" \".join(words)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "6xkerzkMTsvY"}, "outputs": [], "source": [ "x['text_'] = x['text_'].apply(preprocess_text)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "EptlAiZjX7dO"}, "outputs": [], "source": [ "from sentence_transformers import SentenceTransformer\n", "from sklearn.metrics.pairwise import cosine_similarity\n", "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "2oxf6RUKYAUU"}, "outputs": [], "source": [ "import re\n", "\n", "def extract_keyword_using_embeddings(text):\n", " # Ensure the text is valid\n", " if not text or not isinstance(text, str) or not text.strip():\n", " return None # or return an appropriate fallback value\n", "\n", " # Clean the text to retain only words\n", " text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n", " text = text.strip() # Remove leading/trailing whitespace\n", "\n", " # Get the embeddings for the entire tweet\n", " tweet_embedding = model.encode([text])[0]\n", "\n", " # Split the tweet into words and get their embeddings\n", " words = text.split()\n", " if not words: # Handle cases where splitting results in no valid words\n", " return None # or return a fallback value\n", "\n", " word_embeddings = model.encode(words)\n", "\n", " # Calculate the cosine similarity between the tweet embedding and each word embedding\n", " similarities = cosine_similarity([tweet_embedding], word_embeddings)\n", "\n", " # Find the index of the word with the highest similarity\n", " best_word_index = np.argmax(similarities)\n", "\n", " # Return the word that has the highest similarity to the tweet\n", " return words[best_word_index]\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "sWz-jGnUYFEL"}, "outputs": [], "source": [ "#2 - Replacing all rows\n", "x['keyword'] = x['keyword'].replace('', pd.NA)\n", "\n", "# Replace all values in the 'keyword' column with embedding-generated keywords\n", "x['keyword'] = x['text_'].apply(lambda text: extract_keyword_using_embeddings(text))" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "ZcqU3YHlKNSV", "outputId": "b80cd1a0-a780-40da-cf9e-560fc304e651" }, "outputs": [ { "output_type": "stream", "name": "stdout", "text": [ "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n", "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n", "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n", "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n", "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n", "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n", "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n", "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n", "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n", "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n", "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n", "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n", "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n", "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n", "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n", "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n", "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n", "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n", "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n", "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n", "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n", "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n", "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n", "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n", "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n", "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n", "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n" ] } ], "source": [ "pip install datasets" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "0JM2_nsUKQSx"}, "outputs": [], "source": [ "from datasets import Dataset\n", "x['labels']=labels\n", "# Convert to Hugging Face Dataset\n", "hf_dataset = Dataset.from_pandas(data_)" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "_qFw1DVdKSWm"}, "outputs": [], "source": [ "from sklearn.model_selection import train_test_split\n", "\n", "train_data, val_data = train_test_split(x, test_size=0.2, shuffle=True, random_state=42)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "9WuaCIdfKULp"}, "outputs": [], "source": [ "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "4bNwBOHFKV7B"}, "outputs": [], "source": [ "train_dataset = Dataset.from_pandas(train_data)\n", "val_dataset = Dataset.from_pandas(val_data)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "TKI-sUGnKX_v", "outputId": "415002fe-c0f9-4174-e672-7c6c16bef9f7" }, "outputs": [], "source": [ "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "cZxSCOdsKZX5"}, "outputs": [], "source": [ "def tokenize_function(example):\n", " return tokenizer(example[\"text_\"], padding=\"max_length\", truncation=True)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "7ivGFoXhKaia", "outputId": "3375c9f0-f52b-432e-b045-8b83508bbe91" }, "outputs": [], "source": [ "train_dataset = train_dataset.map(tokenize_function, batched=True)\n", "val_dataset = val_dataset.map(tokenize_function, batched=True)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "ssucZhqOKb9q"}, "outputs": [], "source": [ "train_dataset.set_format(\"torch\")\n", "val_dataset.set_format(\"torch\")\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "_53_6jQlKdmY", "outputId": "e4defa50-4c40-4f1b-fc44-0da2642c39f3" }, "outputs": [ { "output_type": "stream", "name": "stderr", "text": [ "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n" ] } ], "source": [ "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=2, hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "hVBPfGKonHzB"}, "outputs": [], "source": [ "#Freeze the first 6 layers of RoBERTa (BERTweet)\n", "for i, layer in enumerate(model.roberta.encoder.layer):\n", " if i < 6: # Adjust number of layers to freeze as needed\n", " for param in layer.parameters():\n", " param.requires_grad = False\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {"id": "dkBRf8iUVZiA"}, "outputs": [], "source": [ "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n", "\n", "def compute_metrics(pred):\n", " \"\"\"\n", " Compute evaluation metrics for the model's predictions.\n", " \"\"\"\n", " labels = pred.label_ids\n", " preds = pred.predictions.argmax(-1) # For classification models with logits\n", " accuracy = accuracy_score(labels, preds)\n", " precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary') # For binary classification\n", " return {\n", " 'accuracy': accuracy,\n", " 'precision': precision,\n", " 'recall': recall,\n", " 'f1': f1\n", " }" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "tAllcEfGKfLz", "outputId": "219187cf-a0dc-495e-b3a3-56e993d12e30" }, "outputs": [ { "output_type": "stream", "name": "stderr", "text": [ "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n", " warnings.warn(\n", ":30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n", " trainer = Trainer(\n" ] } ], "source": [ "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n", "\n", "# Define your training arguments\n", "training_args = TrainingArguments(\n", " output_dir=\"./results\", # Output directory for model checkpoints\n", " evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n", " save_strategy=\"epoch\", # Save the model after each epoch\n", " learning_rate=1e-5,\n", " max_grad_norm=1.0,\n", " per_device_train_batch_size=24, # Training batch size\n", " per_device_eval_batch_size=32, # Validation batch size\n", " num_train_epochs=12, # Total number of epochs\n", " weight_decay=0.03, # Weight decay for regularization\n", " save_total_limit=1, # Save only the last checkpoint\n", " logging_dir=\"./logs\", # Directory for logs\n", " logging_steps=10,\n", " warmup_steps=500,\n", " metric_for_best_model='eval_loss',\n", " greater_is_better=False,\n", " load_best_model_at_end=True,\n", " lr_scheduler_type=\"linear\"\n", ")\n", "\n", "# Add EarlyStoppingCallback to the Trainer\n", "early_stopping = EarlyStoppingCallback(\n", " early_stopping_patience=3 # Number of epochs to wait for improvement in validation loss\n", ")\n", "\n", "# Initialize the Trainer with early stopping callback\n", "trainer = Trainer(\n", " model=model,\n", " args=training_args,\n", " train_dataset=train_dataset,\n", " eval_dataset=val_dataset,\n", " tokenizer=tokenizer,\n", " callbacks=[early_stopping], # Add early stopping here\n", " compute_metrics=compute_metrics\n", ")\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "hCS3fYT-B9_0", "outputId": "b4830ddc-64d6-4623-bb1b-6baff72fadb0" }, "outputs": [ { "output_type": "stream", "name": "stdout", "text": [ "Validation Results: {'eval_loss': 0.280660480260849, 'eval_accuracy': 0.9106521739130434, 'eval_precision': 0.8946378174976481, 'eval_recall': 0.9104834849210148, 'eval_f1': 0.902491103202847, 'eval_runtime': 6.4811, 'eval_samples_per_second': 709.757, 'eval_steps_per_second': 22.218, 'epoch': 9.0}\n" ] } ], "source": [ "trainer.train()\n", "\n", "# Evaluate the model\n", "results = trainer.evaluate()\n", "print(\"Validation Results:\", results)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "l6wwUcN_lzLm", "outputId": "9c2f703a-3767-42f9-aac2-0decf5eeab3a" }, "outputs": [ { "output_type": "stream", "name": "stdout", "text": [ "Model and tokenizer saved to ./results\n" ] } ], "source": [ "import os\n", "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n", "\n", "# Define paths\n", "output_dir = \"./results\"\n", "os.makedirs(output_dir, exist_ok=True)\n", "\n", "# Save model and tokenizer\n", "model.save_pretrained(output_dir)\n", "tokenizer.save_pretrained(output_dir)\n", "\n", "print(f\"Model and tokenizer saved to {output_dir}\")\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": { "id": "c2qrwPti2zUe", "outputId": "160eca2a-a002-45e5-ad24-fb1069ad7373" }, "outputs": [ { "output_type": "stream", "name": "stdout", "text": [ "Precision: 0.9317\n", "Recall: 0.9983\n", "Accuracy: 0.9637\n", "F1 Score: 0.9639\n", "\n", "Misclassified Tweets:\n", "\n", "1. Tweet: Heavy winds and rain expected to last throughout the day. Secure loose outdoor items! #StormAlert #WindWarning\n", "True Label: Positive, Predicted Label: Negative\n", "\n", "2. Tweet: Lost my phone in the parking lot today. Luckily, someone turned it in! #PhoneLost #Thankful\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "3. Tweet: Dropped my favorite mug this morning. RIP to the best coffee mug ever. #AccidentProne #MugDisaster\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "4. Tweet: Slight delay getting to work because of some traffic. Hopefully, this wonâ€™t be a habit! #TrafficTrouble #MorningCommute\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "5. Tweet: Locked myself out of the house today. The locksmith was very kind though! #HouseTroubles #ForgotTheKeys\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "6. Tweet: Lost my favorite jacket in the restaurant. I hope someone finds it! #LostAndFound #JacketWoes\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "7. Tweet: Someone cut me off in traffic this morning. I had to take a deep breath and let it go. #RoadRage #Driving\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "8. Tweet: My friend borrowed my jacket and spilled something on it. Now I need to get it cleaned! #FashionProblems #AccidentsHappen\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "9. Tweet: Someone spilled their drink on the subway this morning. It got messy really quickly! #PublicTransport #MorningCommute\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "10. Tweet: I accidentally left my keys in the car this morning. Had to call a locksmith. #CarProblems #KeyIssues\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "11. Tweet: My car broke down in the middle of the road today. Had to call for a tow! #CarProblems #RoadsideAssistance\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "12. Tweet: I locked myself out of my house and had to call a locksmith. Always something unexpected! #HomeTroubles #LockedOut\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "13. Tweet: Lost my wallet on the bus today. Thankfully, I found it in my bag! #WalletProblems #LostAndFound\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "14. Tweet: Car broke down halfway through my trip. Had to call a mechanic! #CarProblems #RoadsideAssistance\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "15. Tweet: Had a small accident while cooking. The kitchen is a mess, but itâ€™s all good! #CookingFail #KitchenDisaster\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "16. Tweet: Got into a small fender-bender today. Nobody was hurt, just a minor dent! #CarAccident #FenderBender\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "17. Tweet: Dropped my phone in the pool today. It was a total disaster! #PhoneFail #WaterDamage\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "18. Tweet: My car wouldnâ€™t start this morning. Had to call for roadside assistance. #CarTroubles #MorningStruggles\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "19. Tweet: Lost my parking spot today because someone else took it. #ParkingProblems #CityLife\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "20. Tweet: Locked myself out of the house today. Had to call a locksmith. #HomeTroubles #LockedOut\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "21. Tweet: Dropped my bag in the mud today. Now itâ€™s covered in dirt! #BagProblems #AccidentsHappen\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "22. Tweet: Got a small scratch on my car today. Looks like Iâ€™ll need to get it fixed. #CarProblems #MinorDamage\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "23. Tweet: My car was parked in a no-parking zone today. Got a ticket. #ParkingProblems #CityLife\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "24. Tweet: I tripped on the sidewalk today. Thankfully, I didnâ€™t fall! #ClumsyMoments #AccidentsHappen\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "25. Tweet: I lost my favorite jacket in the cafe today. I hope someone returns it! #LostItem #JacketWoes\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "26. Tweet: Got caught in the rain without an umbrella. My shoes are soaked! #RainyDay #Unprepared\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "27. Tweet: Had a small mishap with my blender today. Now the kitchenâ€™s a mess! #KitchenDisaster #BlenderFail\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "28. Tweet: I accidentally left my lights on in the car today. Had to call for a jump start. #CarProblems #TechFail\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "29. Tweet: Spilled some water on my desk today. Itâ€™s a mess now! #DeskProblems #WaterSpill\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "30. Tweet: Locked myself out of my apartment today. Had to call a locksmith. #HomeTroubles #LockedOut\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "31. Tweet: Got locked out of my house this morning. Had to wait for my roommate to let me in! #HomeProblems #LockedOut\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "32. Tweet: I got a flat tire this morning. Had to call a tow truck to fix it. #CarTroubles #FlatTire\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "33. Tweet: The elevator was out of order today. Had to take the stairs! #BuildingProblems #ElevatorFail\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "34. Tweet: Had to take a detour this morning because of a roadblock. It added so much time to my commute! #RoadProblems #CommutingIssues\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "35. Tweet: My car wouldnâ€™t start this morning. Had to call roadside assistance! #CarProblems #MorningTroubles\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "36. Tweet: Tripped over the sidewalk and almost fell. Thankfully, I caught myself! #ClumsyMoments #StreetProblems\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "37. Tweet: Accidentally locked myself out of my car today. Had to wait for roadside assistance. #CarTroubles #LockedOut\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "38. Tweet: Dropped my phone in the sink today. It got completely soaked! #PhoneFail #WaterDamage\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "39. Tweet: Got a paper cut today while working. It hurts more than it looks! #OfficeAccident #PaperCut\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "40. Tweet: Had a small issue with my carâ€™s engine today. It was nothing major, but still inconvenient! #CarProblems #EngineIssues\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "41. Tweet: Got stuck in the elevator today. It took forever for someone to fix it! #ElevatorFail #BuildingProblems\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "42. Tweet: Tripped on the stairs today. Thankfully, I didnâ€™t fall! #ClumsyMoment #Accident\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "43. Tweet: I spilled water on my laptop today. Had to dry it off before it shorted out! #TechFail #WaterSpill\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "44. Tweet: My air conditioner stopped working today. Itâ€™s so hot without it! #HomeProblems #ACFail\n", "True Label: Negative, Predicted Label: Positive\n", "\n", "45. Tweet: Dropped my phone in the pool today. Itâ€™s all wet now! #PhoneFail #WaterDamage\n", "True Label: Negative, Predicted Label: Positive\n" ] } ], "source": [ "import pandas as pd\n", "import torch\n", "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n", "import re\n", "import matplotlib.pyplot as plt\n", "\n", "# Function to detect past events\n", "def is_past_event(text):\n", " past_time_patterns = [\n", " r'\\b\\d+\\s+years? ago\\b',\n", " r'\\b\\d+\\s+months? ago\\b',\n", " r'last\\s+\\w+', # e.g., last week, last year\n", " r'\\bin\\s+\\d{4}\\b', # mentions a specific year\n", " r'\\bformer\\b', # Indicates past events\n", " ]\n", " for pattern in past_time_patterns:\n", " if re.search(pattern, text, flags=re.IGNORECASE):\n", " return True\n", " return False\n", "\n", "# Function to adjust predictions\n", "def adjust_predictions(text, prediction):\n", " if is_past_event(text) and prediction == \"Positive\":\n", " return \"Negative\"\n", " return prediction\n", "\n", "# Load dataset\n", "text_data = pd.read_csv('path/to/final_test.csv')\n", "\n", "# Ensure the dataset has the necessary columns\n", "assert 'tweet' in text_data.columns, \"Column 'tweet' not found in dataset\"\n", "assert 'label' in text_data.columns, \"Column 'label' not found in dataset\"\n", "\n", "# Move model to the appropriate device\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "model.to(device)\n", "\n", "# Tokenize inputs\n", "inputs = tokenizer(\n", " list(text_data['tweet']),\n", " padding=True,\n", " truncation=True,\n", " return_tensors=\"pt\"\n", ").to(device)\n", "\n", "# Get predictions\n", "with torch.no_grad():\n", " outputs = model(**inputs)\n", " logits = outputs.logits\n", "\n", "predictions = torch.argmax(logits, dim=1)\n", "\n", "# Map class indices to labels\n", "class_labels = [\"Negative\", \"Positive\"] # Update as per your model's label mapping\n", "predicted_labels = [class_labels[pred] for pred in predictions.cpu().numpy()]\n", "\n", "# Adjust predictions\n", "adjusted_predictions = [\n", " adjust_predictions(tweet, pred) for tweet, pred in zip(text_data['tweet'], predicted_labels)\n", "]\n", "\n", "# Convert true labels to string format for comparison\n", "true_labels = text_data['label'].map({0: \"Negative\", 1: \"Positive\"}).tolist()\n", "\n", "# Calculate metrics\n", "precision = precision_score(true_labels, adjusted_predictions, pos_label=\"Positive\", average='binary')\n", "recall = recall_score(true_labels, adjusted_predictions, pos_label=\"Positive\", average='binary')\n", "accuracy = accuracy_score(true_labels, adjusted_predictions)\n", "f1 = f1_score(true_labels, adjusted_predictions, pos_label=\"Positive\", average='binary')\n", "\n", "# Display results\n", "print(f\"Precision: {precision:.4f}\")\n", "print(f\"Recall: {recall:.4f}\")\n", "print(f\"Accuracy: {accuracy:.4f}\")\n", "print(f\"F1 Score: {f1:.4f}\")\n", "\n", "# Plot the metrics as bar charts\n", "metrics = [precision, recall, accuracy, f1]\n", "metric_names = ['Precision', 'Recall', 'Accuracy', 'F1 Score']\n", "\n", "misclassifications = []\n", "for tweet, true_label, pred_label in zip(text_data['tweet'], true_labels, adjusted_predictions):\n", " if true_label != pred_label:\n", " misclassifications.append((tweet, true_label, pred_label))\n", "\n", "# Display misclassifications\n", "if misclassifications:\n", " print(\"\\nMisclassified Tweets:\")\n", " for i, (tweet, true_label, pred_label) in enumerate(misclassifications):\n", " print(f\"\\n{i+1}. Tweet: {tweet}\")\n", " print(f\"True Label: {true_label}, Predicted Label: {pred_label}\")\n", "else:\n", " print(\"\\nNo misclassifications found.\")" ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "name": "python3" }, "language_info": { "name": "python" } }, "nbformat": 4, "nbformat_minor": 0 }
